{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDQx5bJosNl_",
        "outputId": "550585a3-73b9-4a5f-ff41-0dfb41728464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ OCR not available. Install with: pip install pytesseract\n",
            "⚠️ PDF processing not available. Install with: pip install PyMuPDF\n",
            "⚠️ DOCX processing not available. Install with: pip install python-docx mammoth\n"
          ]
        }
      ],
      "source": [
        "# Intelligent Document Analysis System\n",
        "# File: document_analyzer.py\n",
        "\n",
        "\"\"\"\n",
        "Advanced Document Analysis System with AI\n",
        "=========================================\n",
        "\n",
        "Features:\n",
        "- Multi-format document processing (PDF, DOCX, images, text)\n",
        "- OCR for image-based documents\n",
        "- Named Entity Recognition (NER)\n",
        "- Document classification\n",
        "- Sentiment analysis\n",
        "- Automatic summarization\n",
        "- Keyword extraction\n",
        "- Web interface with FastAPI\n",
        "\n",
        "Requirements:\n",
        "pip install fastapi uvicorn python-multipart\n",
        "pip install pillow pytesseract opencv-python\n",
        "pip install transformers torch sentence-transformers\n",
        "pip install PyMuPDF python-docx mammoth\n",
        "pip install spacy && python -m spacy download en_core_web_sm\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "import time\n",
        "import hashlib\n",
        "\n",
        "# Core libraries\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# OCR\n",
        "try:\n",
        "    import pytesseract\n",
        "    OCR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OCR_AVAILABLE = False\n",
        "    print(\"⚠️ OCR not available. Install with: pip install pytesseract\")\n",
        "\n",
        "# Document processing\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    PDF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PDF_AVAILABLE = False\n",
        "    print(\"⚠️ PDF processing not available. Install with: pip install PyMuPDF\")\n",
        "\n",
        "try:\n",
        "    from docx import Document as DocxDocument\n",
        "    import mammoth\n",
        "    DOCX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DOCX_AVAILABLE = False\n",
        "    print(\"⚠️ DOCX processing not available. Install with: pip install python-docx mammoth\")\n",
        "\n",
        "# NLP libraries\n",
        "try:\n",
        "    import spacy\n",
        "    NLP_AVAILABLE = True\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        nlp = None\n",
        "        print(\"⚠️ spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n",
        "except ImportError:\n",
        "    NLP_AVAILABLE = False\n",
        "    nlp = None\n",
        "    print(\"⚠️ spaCy not available. Install with: pip install spacy\")\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"⚠️ Transformers not available. Install with: pip install transformers torch\")\n",
        "\n",
        "# Web framework\n",
        "try:\n",
        "    from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "    from fastapi.responses import HTMLResponse, JSONResponse\n",
        "    from fastapi.middleware.cors import CORSMiddleware\n",
        "    import uvicorn\n",
        "    WEB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WEB_AVAILABLE = False\n",
        "    print(\"⚠️ Web framework not available. Install with: pip install fastapi uvicorn python-multipart\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DocumentAnalyzer:\n",
        "    \"\"\"\n",
        "    Advanced Document Analysis System\n",
        "\n",
        "    Capabilities:\n",
        "    - Text extraction from multiple formats\n",
        "    - OCR for images\n",
        "    - Named Entity Recognition\n",
        "    - Document classification\n",
        "    - Sentiment analysis\n",
        "    - Summarization\n",
        "    - Keyword extraction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the document analyzer with AI models\"\"\"\n",
        "        logger.info(\"🚀 Initializing Document Analyzer...\")\n",
        "\n",
        "        # Initialize components\n",
        "        self.nlp_model = nlp if NLP_AVAILABLE else None\n",
        "        self.initialize_ai_models()\n",
        "\n",
        "        # Document type mappings\n",
        "        self.supported_formats = {\n",
        "            'pdf': self.extract_text_from_pdf,\n",
        "            'docx': self.extract_text_from_docx,\n",
        "            'doc': self.extract_text_from_docx,\n",
        "            'txt': self.extract_text_from_txt,\n",
        "            'png': self.extract_text_from_image,\n",
        "            'jpg': self.extract_text_from_image,\n",
        "            'jpeg': self.extract_text_from_image,\n",
        "            'tiff': self.extract_text_from_image,\n",
        "            'bmp': self.extract_text_from_image\n",
        "        }\n",
        "\n",
        "        logger.info(\"✅ Document Analyzer initialized successfully!\")\n",
        "\n",
        "    def initialize_ai_models(self):\n",
        "        \"\"\"Initialize AI models for analysis\"\"\"\n",
        "        self.sentiment_analyzer = None\n",
        "        self.summarizer = None\n",
        "        self.classifier = None\n",
        "\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            try:\n",
        "                # Sentiment analysis\n",
        "                self.sentiment_analyzer = pipeline(\n",
        "                    \"sentiment-analysis\",\n",
        "                    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "                )\n",
        "\n",
        "                # Summarization\n",
        "                self.summarizer = pipeline(\n",
        "                    \"summarization\",\n",
        "                    model=\"facebook/bart-large-cnn\"\n",
        "                )\n",
        "\n",
        "                # Document classification\n",
        "                self.classifier = pipeline(\n",
        "                    \"zero-shot-classification\",\n",
        "                    model=\"facebook/bart-large-mnli\"\n",
        "                )\n",
        "\n",
        "                logger.info(\"✅ AI models loaded successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"⚠️ Some AI models couldn't be loaded: {e}\")\n",
        "\n",
        "    def get_file_hash(self, content: bytes) -> str:\n",
        "        \"\"\"Generate hash for file content\"\"\"\n",
        "        return hashlib.md5(content).hexdigest()\n",
        "\n",
        "    def detect_file_type(self, filename: str) -> str:\n",
        "        \"\"\"Detect file type from filename\"\"\"\n",
        "        return filename.split('.')[-1].lower() if '.' in filename else 'unknown'\n",
        "\n",
        "    def extract_text_from_pdf(self, content: bytes) -> str:\n",
        "        \"\"\"Extract text from PDF using PyMuPDF\"\"\"\n",
        "        if not PDF_AVAILABLE:\n",
        "            return \"PDF processing not available\"\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(stream=content, filetype=\"pdf\")\n",
        "            text = \"\"\n",
        "            for page_num in range(doc.page_count):\n",
        "                page = doc[page_num]\n",
        "                text += page.get_text()\n",
        "            doc.close()\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting PDF text: {e}\")\n",
        "            return f\"Error processing PDF: {e}\"\n",
        "\n",
        "    def extract_text_from_docx(self, content: bytes) -> str:\n",
        "        \"\"\"Extract text from DOCX file\"\"\"\n",
        "        if not DOCX_AVAILABLE:\n",
        "            return \"DOCX processing not available\"\n",
        "\n",
        "        try:\n",
        "            # Try mammoth first (better formatting)\n",
        "            result = mammoth.extract_raw_text(io.BytesIO(content))\n",
        "            return result.value.strip()\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                # Fallback to python-docx\n",
        "                doc = DocxDocument(io.BytesIO(content))\n",
        "                text = []\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text.append(paragraph.text)\n",
        "                return '\\n'.join(text).strip()\n",
        "            except Exception as e2:\n",
        "                logger.error(f\"Error extracting DOCX text: {e}, {e2}\")\n",
        "                return f\"Error processing DOCX: {e}\"\n",
        "\n",
        "    def extract_text_from_txt(self, content: bytes) -> str:\n",
        "        \"\"\"Extract text from plain text file\"\"\"\n",
        "        try:\n",
        "            return content.decode('utf-8', errors='ignore').strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting text: {e}\")\n",
        "            return f\"Error processing text file: {e}\"\n",
        "\n",
        "    def extract_text_from_image(self, content: bytes) -> str:\n",
        "        \"\"\"Extract text from image using OCR\"\"\"\n",
        "        if not OCR_AVAILABLE:\n",
        "            return \"OCR not available\"\n",
        "\n",
        "        try:\n",
        "            # Convert bytes to PIL Image\n",
        "            image = Image.open(io.BytesIO(content))\n",
        "\n",
        "            # Convert to numpy array for OpenCV processing\n",
        "            img_array = np.array(image)\n",
        "\n",
        "            # Convert RGB to BGR for OpenCV\n",
        "            if len(img_array.shape) == 3:\n",
        "                img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
        "                # Convert to grayscale\n",
        "                gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
        "            else:\n",
        "                gray = img_array\n",
        "\n",
        "            # Image preprocessing for better OCR\n",
        "            # Apply Gaussian blur\n",
        "            blurred = cv2.GaussianBlur(gray, (1, 1), 0)\n",
        "\n",
        "            # Apply threshold to get binary image\n",
        "            _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "            # Convert back to PIL Image\n",
        "            processed_image = Image.fromarray(thresh)\n",
        "\n",
        "            # OCR configuration\n",
        "            custom_config = r'--oem 3 --psm 6'\n",
        "            text = pytesseract.image_to_string(processed_image, config=custom_config)\n",
        "\n",
        "            return text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting text from image: {e}\")\n",
        "            return f\"Error processing image: {e}\"\n",
        "\n",
        "    def extract_text(self, content: bytes, file_type: str) -> str:\n",
        "        \"\"\"Extract text based on file type\"\"\"\n",
        "        if file_type in self.supported_formats:\n",
        "            return self.supported_formats[file_type](content)\n",
        "        else:\n",
        "            return f\"Unsupported file type: {file_type}\"\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract named entities using spaCy\"\"\"\n",
        "        if not self.nlp_model or not text.strip():\n",
        "            return {\"message\": \"NLP model not available or empty text\"}\n",
        "\n",
        "        try:\n",
        "            # Limit text length for processing\n",
        "            doc = self.nlp_model(text[:100000])\n",
        "\n",
        "            entities = {}\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ not in entities:\n",
        "                    entities[ent.label_] = []\n",
        "                if ent.text not in entities[ent.label_]:\n",
        "                    entities[ent.label_].append(ent.text)\n",
        "\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting entities: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze sentiment of the text\"\"\"\n",
        "        if not self.sentiment_analyzer or not text.strip():\n",
        "            return {\"message\": \"Sentiment analyzer not available or empty text\"}\n",
        "\n",
        "        try:\n",
        "            # Limit text length for processing\n",
        "            text_sample = text[:512]\n",
        "            result = self.sentiment_analyzer(text_sample)[0]\n",
        "\n",
        "            return {\n",
        "                \"label\": result[\"label\"],\n",
        "                \"confidence\": round(result[\"score\"], 4),\n",
        "                \"text_sample\": text_sample[:100] + \"...\" if len(text_sample) > 100 else text_sample\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing sentiment: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def classify_document(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Classify document into categories\"\"\"\n",
        "        if not self.classifier or not text.strip():\n",
        "            return {\"message\": \"Classifier not available or empty text\"}\n",
        "\n",
        "        try:\n",
        "            # Define document categories\n",
        "            categories = [\n",
        "                \"legal document\", \"medical report\", \"financial document\",\n",
        "                \"technical manual\", \"business letter\", \"academic paper\",\n",
        "                \"news article\", \"personal letter\", \"contract\", \"invoice\"\n",
        "            ]\n",
        "\n",
        "            # Limit text length for processing\n",
        "            text_sample = text[:1024]\n",
        "            result = self.classifier(text_sample, categories)\n",
        "\n",
        "            return {\n",
        "                \"category\": result[\"labels\"][0],\n",
        "                \"confidence\": round(result[\"scores\"][0], 4),\n",
        "                \"all_scores\": {\n",
        "                    label: round(score, 4)\n",
        "                    for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error classifying document: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def summarize_text(self, text: str, max_length: int = 150) -> str:\n",
        "        \"\"\"Generate summary of the text\"\"\"\n",
        "        if not self.summarizer or not text.strip():\n",
        "            return \"Summarizer not available or empty text\"\n",
        "\n",
        "        try:\n",
        "            # Ensure text is long enough to summarize\n",
        "            if len(text.split()) < 50:\n",
        "                return \"Text too short to summarize effectively\"\n",
        "\n",
        "            # Limit text length for processing\n",
        "            text_sample = text[:1024]\n",
        "\n",
        "            result = self.summarizer(\n",
        "                text_sample,\n",
        "                max_length=max_length,\n",
        "                min_length=30,\n",
        "                do_sample=False\n",
        "            )[0]\n",
        "\n",
        "            return result[\"summary_text\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error summarizing text: {e}\")\n",
        "            return f\"Error generating summary: {e}\"\n",
        "\n",
        "    def extract_keywords(self, text: str, num_keywords: int = 10) -> List[str]:\n",
        "        \"\"\"Extract keywords using simple frequency analysis\"\"\"\n",
        "        if not text.strip():\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Simple keyword extraction using word frequency\n",
        "            words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
        "\n",
        "            # Remove common stop words\n",
        "            stop_words = {\n",
        "                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',\n",
        "                'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',\n",
        "                'after', 'above', 'below', 'between', 'among', 'this', 'that', 'these',\n",
        "                'those', 'his', 'her', 'their', 'our', 'your', 'its', 'his', 'him',\n",
        "                'she', 'they', 'we', 'you', 'are', 'was', 'were', 'been', 'be',\n",
        "                'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
        "                'should', 'may', 'might', 'can', 'said', 'say', 'get', 'go', 'come'\n",
        "            }\n",
        "\n",
        "            # Filter out stop words and count frequency\n",
        "            word_freq = {}\n",
        "            for word in words:\n",
        "                if word not in stop_words and len(word) > 3:\n",
        "                    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "            # Sort by frequency and return top keywords\n",
        "            keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "            return [word for word, freq in keywords[:num_keywords]]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting keywords: {e}\")\n",
        "            return []\n",
        "\n",
        "    def analyze_document(self, content: bytes, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete document analysis pipeline\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Basic file info\n",
        "        file_type = self.detect_file_type(filename)\n",
        "        file_hash = self.get_file_hash(content)\n",
        "        file_size = len(content)\n",
        "\n",
        "        # Extract text\n",
        "        logger.info(f\"Extracting text from {file_type} file...\")\n",
        "        extracted_text = self.extract_text(content, file_type)\n",
        "\n",
        "        if extracted_text.startswith(\"Error\") or not extracted_text.strip():\n",
        "            return {\n",
        "                \"filename\": filename,\n",
        "                \"file_type\": file_type,\n",
        "                \"file_size\": file_size,\n",
        "                \"file_hash\": file_hash,\n",
        "                \"error\": \"Failed to extract text from document\",\n",
        "                \"processing_time\": time.time() - start_time\n",
        "            }\n",
        "\n",
        "        # Perform analysis\n",
        "        logger.info(\"Performing NLP analysis...\")\n",
        "\n",
        "        # Extract entities\n",
        "        entities = self.extract_entities(extracted_text)\n",
        "\n",
        "        # Analyze sentiment\n",
        "        sentiment = self.analyze_sentiment(extracted_text)\n",
        "\n",
        "        # Classify document\n",
        "        classification = self.classify_document(extracted_text)\n",
        "\n",
        "        # Generate summary\n",
        "        summary = self.summarize_text(extracted_text)\n",
        "\n",
        "        # Extract keywords\n",
        "        keywords = self.extract_keywords(extracted_text)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Compile results\n",
        "        result = {\n",
        "            \"filename\": filename,\n",
        "            \"file_type\": file_type,\n",
        "            \"file_size\": file_size,\n",
        "            \"file_hash\": file_hash,\n",
        "            \"text_length\": len(extracted_text),\n",
        "            \"extracted_text\": extracted_text[:500] + \"...\" if len(extracted_text) > 500 else extracted_text,\n",
        "            \"entities\": entities,\n",
        "            \"sentiment\": sentiment,\n",
        "            \"classification\": classification,\n",
        "            \"summary\": summary,\n",
        "            \"keywords\": keywords,\n",
        "            \"processing_time\": round(processing_time, 2),\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Analysis completed in {processing_time:.2f} seconds\")\n",
        "        return result\n",
        "\n",
        "# Web Application (if FastAPI is available)\n",
        "if WEB_AVAILABLE:\n",
        "    app = FastAPI(title=\"Document Analyzer API\", version=\"1.0.0\")\n",
        "\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = DocumentAnalyzer()\n",
        "\n",
        "    @app.get(\"/\", response_class=HTMLResponse)\n",
        "    async def home():\n",
        "        \"\"\"Web interface for document upload\"\"\"\n",
        "        html_content = \"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>📄 Document Analyzer</title>\n",
        "            <style>\n",
        "                body {\n",
        "                    font-family: Arial, sans-serif;\n",
        "                    max-width: 1200px;\n",
        "                    margin: 0 auto;\n",
        "                    padding: 20px;\n",
        "                    background: #f5f5f5;\n",
        "                }\n",
        "                .container {\n",
        "                    background: white;\n",
        "                    padding: 30px;\n",
        "                    border-radius: 10px;\n",
        "                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
        "                }\n",
        "                h1 { color: #333; text-align: center; }\n",
        "                .upload-area {\n",
        "                    border: 2px dashed #ddd;\n",
        "                    border-radius: 10px;\n",
        "                    padding: 40px;\n",
        "                    text-align: center;\n",
        "                    margin: 20px 0;\n",
        "                    background: #fafafa;\n",
        "                }\n",
        "                .upload-area:hover { border-color: #007bff; }\n",
        "                .file-input {\n",
        "                    display: none;\n",
        "                }\n",
        "                .upload-btn {\n",
        "                    background: #007bff;\n",
        "                    color: white;\n",
        "                    padding: 12px 24px;\n",
        "                    border: none;\n",
        "                    border-radius: 5px;\n",
        "                    cursor: pointer;\n",
        "                    font-size: 16px;\n",
        "                }\n",
        "                .upload-btn:hover { background: #0056b3; }\n",
        "                .analyze-btn {\n",
        "                    background: #28a745;\n",
        "                    color: white;\n",
        "                    padding: 12px 24px;\n",
        "                    border: none;\n",
        "                    border-radius: 5px;\n",
        "                    cursor: pointer;\n",
        "                    font-size: 16px;\n",
        "                    margin-top: 10px;\n",
        "                }\n",
        "                .analyze-btn:hover { background: #1e7e34; }\n",
        "                .results {\n",
        "                    margin-top: 30px;\n",
        "                    padding: 20px;\n",
        "                    background: #f8f9fa;\n",
        "                    border-radius: 5px;\n",
        "                }\n",
        "                .loading {\n",
        "                    display: none;\n",
        "                    text-align: center;\n",
        "                    color: #007bff;\n",
        "                    font-size: 18px;\n",
        "                }\n",
        "                .error { color: #dc3545; }\n",
        "                .success { color: #28a745; }\n",
        "                .info-card {\n",
        "                    background: white;\n",
        "                    padding: 15px;\n",
        "                    margin: 10px 0;\n",
        "                    border-radius: 5px;\n",
        "                    border-left: 4px solid #007bff;\n",
        "                }\n",
        "                .entity-tag {\n",
        "                    display: inline-block;\n",
        "                    background: #e9ecef;\n",
        "                    padding: 4px 8px;\n",
        "                    margin: 2px;\n",
        "                    border-radius: 3px;\n",
        "                    font-size: 12px;\n",
        "                }\n",
        "                .keyword-tag {\n",
        "                    display: inline-block;\n",
        "                    background: #d4edda;\n",
        "                    padding: 4px 8px;\n",
        "                    margin: 2px;\n",
        "                    border-radius: 3px;\n",
        "                    font-size: 12px;\n",
        "                }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"container\">\n",
        "                <h1>📄 AI Document Analyzer</h1>\n",
        "                <p style=\"text-align: center; color: #666;\">\n",
        "                    Upload documents for intelligent analysis including OCR, NER, sentiment analysis, and summarization\n",
        "                </p>\n",
        "\n",
        "                <div class=\"upload-area\" onclick=\"document.getElementById('fileInput').click()\">\n",
        "                    <div id=\"uploadText\">\n",
        "                        <h3>📁 Click to upload document</h3>\n",
        "                        <p>Supports: PDF, DOCX, TXT, Images (PNG, JPG, etc.)</p>\n",
        "                    </div>\n",
        "                    <input type=\"file\" id=\"fileInput\" class=\"file-input\" accept=\".pdf,.docx,.doc,.txt,.png,.jpg,.jpeg,.tiff,.bmp\">\n",
        "                </div>\n",
        "\n",
        "                <div style=\"text-align: center;\">\n",
        "                    <button class=\"analyze-btn\" onclick=\"analyzeDocument()\" id=\"analyzeBtn\" disabled>\n",
        "                        🔍 Analyze Document\n",
        "                    </button>\n",
        "                </div>\n",
        "\n",
        "                <div class=\"loading\" id=\"loading\">\n",
        "                    <h3>🔄 Analyzing document...</h3>\n",
        "                    <p>This may take a few moments depending on document size and complexity.</p>\n",
        "                </div>\n",
        "\n",
        "                <div class=\"results\" id=\"results\" style=\"display: none;\"></div>\n",
        "            </div>\n",
        "\n",
        "            <script>\n",
        "                let selectedFile = null;\n",
        "\n",
        "                document.getElementById('fileInput').addEventListener('change', function(e) {\n",
        "                    selectedFile = e.target.files[0];\n",
        "                    if (selectedFile) {\n",
        "                        document.getElementById('uploadText').innerHTML =\n",
        "                            '<h3>✅ File selected: ' + selectedFile.name + '</h3>' +\n",
        "                            '<p>Size: ' + (selectedFile.size / 1024 / 1024).toFixed(2) + ' MB</p>';\n",
        "                        document.getElementById('analyzeBtn').disabled = false;\n",
        "                    }\n",
        "                });\n",
        "\n",
        "                async function analyzeDocument() {\n",
        "                    if (!selectedFile) {\n",
        "                        alert('Please select a file first!');\n",
        "                        return;\n",
        "                    }\n",
        "\n",
        "                    const formData = new FormData();\n",
        "                    formData.append('file', selectedFile);\n",
        "\n",
        "                    document.getElementById('loading').style.display = 'block';\n",
        "                    document.getElementById('results').style.display = 'none';\n",
        "                    document.getElementById('analyzeBtn').disabled = true;\n",
        "\n",
        "                    try {\n",
        "                        const response = await fetch('/analyze', {\n",
        "                            method: 'POST',\n",
        "                            body: formData\n",
        "                        });\n",
        "\n",
        "                        const data = await response.json();\n",
        "                        displayResults(data);\n",
        "\n",
        "                    } catch (error) {\n",
        "                        document.getElementById('results').innerHTML =\n",
        "                            '<div class=\"error\"><h3>❌ Error</h3><p>' + error.message + '</p></div>';\n",
        "                        document.getElementById('results').style.display = 'block';\n",
        "                    }\n",
        "\n",
        "                    document.getElementById('loading').style.display = 'none';\n",
        "                    document.getElementById('analyzeBtn').disabled = false;\n",
        "                }\n",
        "\n",
        "                function displayResults(data) {\n",
        "                    let html = '<h3>📊 Analysis Results</h3>';\n",
        "\n",
        "                    if (data.error) {\n",
        "                        html += '<div class=\"error\"><h4>❌ Error</h4><p>' + data.error + '</p></div>';\n",
        "                    } else {\n",
        "                        // File info\n",
        "                        html += '<div class=\"info-card\">';\n",
        "                        html += '<h4>📋 File Information</h4>';\n",
        "                        html += '<p><strong>Filename:</strong> ' + data.filename + '</p>';\n",
        "                        html += '<p><strong>Type:</strong> ' + data.file_type.toUpperCase() + '</p>';\n",
        "                        html += '<p><strong>Size:</strong> ' + (data.file_size / 1024).toFixed(2) + ' KB</p>';\n",
        "                        html += '<p><strong>Text Length:</strong> ' + data.text_length + ' characters</p>';\n",
        "                        html += '<p><strong>Processing Time:</strong> ' + data.processing_time + 's</p>';\n",
        "                        html += '</div>';\n",
        "\n",
        "                        // Classification\n",
        "                        if (data.classification && data.classification.category) {\n",
        "                            html += '<div class=\"info-card\">';\n",
        "                            html += '<h4>📂 Document Classification</h4>';\n",
        "                            html += '<p><strong>Category:</strong> ' + data.classification.category + '</p>';\n",
        "                            html += '<p><strong>Confidence:</strong> ' + (data.classification.confidence * 100).toFixed(1) + '%</p>';\n",
        "                            html += '</div>';\n",
        "                        }\n",
        "\n",
        "                        // Sentiment\n",
        "                        if (data.sentiment && data.sentiment.label) {\n",
        "                            html += '<div class=\"info-card\">';\n",
        "                            html += '<h4>😊 Sentiment Analysis</h4>';\n",
        "                            html += '<p><strong>Sentiment:</strong> ' + data.sentiment.label + '</p>';\n",
        "                            html += '<p><strong>Confidence:</strong> ' + (data.sentiment.confidence * 100).toFixed(1) + '%</p>';\n",
        "                            html += '</div>';\n",
        "                        }\n",
        "\n",
        "                        // Summary\n",
        "                        if (data.summary && !data.summary.startsWith('Error') && !data.summary.includes('not available')) {\n",
        "                            html += '<div class=\"info-card\">';\n",
        "                            html += '<h4>📝 Summary</h4>';\n",
        "                            html += '<p>' + data.summary + '</p>';\n",
        "                            html += '</div>';\n",
        "                        }\n",
        "\n",
        "                        // Keywords\n",
        "                        if (data.keywords && data.keywords.length > 0) {\n",
        "                            html += '<div class=\"info-card\">';\n",
        "                            html += '<h4>🔑 Keywords</h4>';\n",
        "                            data.keywords.forEach(keyword => {\n",
        "                                html += '<span class=\"keyword-tag\">' + keyword + '</span>';\n",
        "                            });\n",
        "                            html += '</div>';\n",
        "                        }\n",
        "\n",
        "                        // Entities\n",
        "                        if (data.entities && Object.keys(data.entities).length > 0 && !data.entities.message) {\n",
        "                            html += '<div class=\"info-card\">';\n",
        "                            html += '<h4>🏷️ Named Entities</h4>';\n",
        "                            for (const [label, entities] of Object.entries(data.entities)) {\n",
        "                                if (entities.length > 0) {\n",
        "                                    html += '<p><strong>' + label + ':</strong></p>';\n",
        "                                    entities.forEach(entity => {\n",
        "                                        html += '<span class=\"entity-tag\">' + entity + '</span>';\n",
        "                                    });\n",
        "                                }\n",
        "                            }\n",
        "                            html += '</div>';\n",
        "                        }\n",
        "\n",
        "                        // Extracted text preview\n",
        "                        if (data.extracted_text) {\n",
        "                            html += '<div class=\"info-card\">';\n",
        "                            html += '<h4>📄 Text Preview</h4>';\n",
        "                            html += '<p style=\"max-height: 200px; overflow-y: auto; background: #f8f9fa; padding: 10px; border-radius: 3px;\">' +\n",
        "                                    data.extracted_text.replace(/\\n/g, '<br>') + '</p>';\n",
        "                            html += '</div>';\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                    document.getElementById('results').innerHTML = html;\n",
        "                    document.getElementById('results').style.display = 'block';\n",
        "                }\n",
        "            </script>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "        return html_content\n",
        "\n",
        "    @app.post(\"/analyze\")\n",
        "    async def analyze_document(file: UploadFile = File(...)):\n",
        "        \"\"\"Analyze uploaded document\"\"\"\n",
        "        try:\n",
        "            # Read file content\n",
        "            content = await file.read()\n",
        "\n",
        "            # Analyze document\n",
        "            result = analyzer.analyze_document(content, file.filename)\n",
        "\n",
        "            return JSONResponse(content=result)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing document: {e}\")\n",
        "            raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "    @app.get(\"/health\")\n",
        "    async def health_check():\n",
        "        \"\"\"Health check endpoint\"\"\"\n",
        "        return {\n",
        "            \"status\": \"healthy\",\n",
        "            \"service\": \"document_analyzer\",\n",
        "            \"features\": {\n",
        "                \"pdf_processing\": PDF_AVAILABLE,\n",
        "                \"docx_processing\": DOCX_AVAILABLE,\n",
        "                \"ocr\": OCR_AVAILABLE,\n",
        "                \"nlp\": NLP_AVAILABLE,\n",
        "                \"transformers\": TRANSFORMERS_AVAILABLE\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Command Line Interface\n",
        "def main():\n",
        "    \"\"\"Main function for command line usage\"\"\"\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"📄 Document Analyzer\")\n",
        "        print(\"===================\")\n",
        "        print()\n",
        "        print(\"Usage:\")\n",
        "        print(\"  python document_analyzer.py <file_path>     # Analyze single file\")\n",
        "        print(\"  python document_analyzer.py --web           # Start web server\")\n",
        "        print()\n",
        "        print(\"Examples:\")\n",
        "        print(\"  python document_analyzer.py document.pdf\")\n",
        "        print(\"  python document_analyzer.py --web\")\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if sys.argv[1] == \"--web\":\n",
        "        if WEB_AVAILABLE:\n",
        "            print(\"🚀 Starting Document Analyzer Web Server...\")\n",
        "            print(\"📱 Access at: http://localhost:8000\")\n",
        "            uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
        "        else:\n",
        "            print(\"❌ Web server not available. Install with: pip install fastapi uvicorn\")\n",
        "        return\n",
        "\n",
        "    # Analyze file from command line\n",
        "    file_path = sys.argv[1]\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"❌ File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Read file\n",
        "        with open(file_path, 'rb') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Initialize analyzer\n",
        "        analyzer = DocumentAnalyzer()\n",
        "\n",
        "        # Analyze document\n",
        "        print(f\"🔍 Analyzing: {file_path}\")\n",
        "        result = analyzer.analyze_document(content, os.path.basename(file_path))\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n📊 Analysis Results\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        if \"error\" in result:\n",
        "            print(f\"❌ Error: {result['error']}\")\n",
        "            return\n",
        "\n",
        "        print(f\"📄 File: {result['filename']}\")\n",
        "        print(f\"📁 Type: {result['file_type'].upper()}\")\n",
        "        print(f\"📏 Size: {result['file_size']:,} bytes\")\n",
        "        print(f\"📝 Text Length: {result['text_length']:,} characters\")\n",
        "        print(f\"⏱️ Processing Time: {result['processing_time']}s\")\n",
        "\n",
        "        if result.get('classification', {}).get('category'):\n",
        "            print(f\"\\n📂 Classification: {result['classification']['category']}\")\n",
        "            print(f\"   Confidence: {result['classification']['confidence']:.2%}\")\n",
        "\n",
        "        if result.get('sentiment', {}).get('label'):\n",
        "            print(f\"\\n😊 Sentiment: {result['sentiment']['label']}\")\n",
        "            print(f\"   Confidence: {result['sentiment']['confidence']:.2%}\")\n",
        "\n",
        "        if result.get('summary') and not result['summary'].startswith('Error'):\n",
        "            print(f\"\\n📝 Summary:\")\n",
        "            print(f\"   {result['summary']}\")\n",
        "\n",
        "        if result.get('keywords'):\n",
        "            print(f\"\\n🔑 Keywords: {', '.join(result['keywords'][:10])}\")\n",
        "\n",
        "        if result.get('entities') and not result['entities'].get('message'):\n",
        "            print(f\"\\n🏷️ Named Entities:\")\n",
        "            for label, entities in result['entities'].items():\n",
        "                if entities:\n",
        "                    print(f\"   {label}: {', '.join(entities[:5])}\")\n",
        "\n",
        "        print(f\"\\n📄 Text Preview:\")\n",
        "        preview = result['extracted_text'][:300]\n",
        "        print(f\"   {preview}{'...' if len(result['extracted_text']) > 300 else ''}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}